{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import lxml\n",
    "import pandas as pd\n",
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for thread\n",
    "\n",
    "# locker\n",
    "threadLock = threading.Lock()\n",
    "\n",
    "class MyThread(threading.Thread):\n",
    "    def __init__(self, processes, result, article, tag):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.processes = processes\n",
    "        self.result = result\n",
    "        self.article = article\n",
    "        self.tag = tag\n",
    "    def run(self):\n",
    "        temp = []\n",
    "        for process in self.processes:\n",
    "            temp.append(process(self.article))\n",
    "        temp.append(self.tag)\n",
    "        threadLock.acquire()\n",
    "        self.result.append(temp)\n",
    "        threadLock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding header of the article\n",
    "def find_header(article):\n",
    "    return (article.find(\"h2\", class_ = \"entry-title\")).text\n",
    "\n",
    "# finding full text of the article\n",
    "def find_article_text(article):\n",
    "    # search for url to the text\n",
    "    text_url = article.find(\"div\", class_ = \"np-article-thumb\").find(\"a\").get(\"href\")\n",
    "    \n",
    "    # connect to new html page\n",
    "    temp_response = requests.get(text_url)\n",
    "    temp_soup = BeautifulSoup(temp_response.text, \"lxml\")\n",
    "    \n",
    "    # search full text of article\n",
    "    text_list = []\n",
    "    for item in (temp_soup.find(\"div\", class_ = \"entry-content\")).find_all(\"p\"):\n",
    "        text_list.append(item.text)\n",
    "\n",
    "    # make one text from the list\n",
    "    article_text = \" \".join(text_list)\n",
    "    \n",
    "    # return result\n",
    "    return article_text\n",
    "\n",
    "# finding date of the article\n",
    "def find_date(article):\n",
    "    return (article.find(\"time\")).text\n",
    "\n",
    "# main func for finding articles\n",
    "def find_information(url, tag, num_of_pages, result):\n",
    "    url += tag + \"/\"\n",
    "    \n",
    "    for i in range(1, num_of_pages + 1):\n",
    "        # change page (first page without page/1)\n",
    "        if (i > 1):\n",
    "            new_url = url + \"page/{0}\".format(str(i))\n",
    "        else:\n",
    "            new_url = url\n",
    "            \n",
    "        # connect to the page\n",
    "        response = requests.get(new_url)\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "        \n",
    "        # get all articles from the page\n",
    "        articles = soup.find_all(\"article\")\n",
    "        \n",
    "        # create threads for each article\n",
    "        processes = [find_header, find_article_text, find_date]\n",
    "        threads = [MyThread(processes, result, articles[i], tag) for i in range(len(articles))]\n",
    "        \n",
    "        # start running threads\n",
    "        for t in threads:\n",
    "            t.start()\n",
    "            \n",
    "        # waiting for every thread\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "    \n",
    "    \n",
    "            \n",
    "\n",
    "url = 'https://panorama.pub/category/news/' # + tag\n",
    "tags = ['politics', 'society', 'science', 'economics', 'books']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# politics 129\n",
    "# society 316\n",
    "# science 27\n",
    "# economics 28\n",
    "# books 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1393.9433810710907 seconds ---\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "start_time = time.time()\n",
    "find_information(url, \"politics\", 129, result)\n",
    "find_information(url, \"society\", 316, result)\n",
    "find_information(url, \"science\", 27, result)\n",
    "find_information(url, \"economics\", 28, result)\n",
    "find_information(url, \"books\", 1, result)\n",
    "print (\"--- {0} seconds ---\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (pd.DataFrame(data=result, columns=['title', 'text', 'date', 'tag'])).sample(frac=1).reset_index(drop=True)\n",
    "data.to_excel(\"C:/Users/semav/Desktop/result.xlsx\", sheet_name=\"articles_panorama\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
